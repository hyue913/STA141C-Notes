---
title: "Untitled"
author: "Hangyu Yue"
date: "2/4/2020"
output: html_document
---
```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(cache = TRUE)
```

```{r, message = TRUE}
library(tidyverse)
```

# Parallel Computation


## Implict Parallel

- BLAS (Basic Linear Algebra Subroutines)
  - CRAN R shippings with a [version](https://github.com/wch/r-source/tree/trunk/src/extra/blas) of single threaded BLAS library.
  - [Microsoft R Open](https://mran.microsoft.com/open) ships with Interl MKL (Win/Linux) / Accelerate ML (macOS) BLAS libraries.
  - on macOS, R could be configured to use the optimized BLAS from Appleâ€™s Accelerate framework
  - We could only install R with different BLAS libraries such as [openblas](https://github.com/xianyi/OpenBLAS) or [ATLAS](http://math-atlas.sourceforge.net/)


## Embarrassingly Parallel

Also called perfectly parallel, delightfully parallel or pleasingly parallel. 

> An embarrassingly parallel task can be considered a trivial case - little or no manipulation is needed to separate the problem into a number of parallel tasks.
A bit deroute first - revisit some of our old friends `map` and `map_*` in `purrr`.

```{r}
1:4 %>% map(function(x) x^2)
1:4 %>% map_dbl(function(x) x^2)
```

These are the base R equivalence.
```{r}
1:4 %>% lapply(function(x) x^2)
1:4 %>% sapply(function(x) x^2)
```


Suppose we have a list of vectors and we want to operation some operation on each vector.

```{r}
# it is a slow operation, imagine that in real applications, it could take a few minutes
slow_task <- function(x) {
  sum(x %o% x)
}
list_of_vectors <- replicate(10, list(rnorm(5000)))
list_of_vectors %>% glimpse()
```





```{r, eval = FALSE}
list_of_vectors %>% map_dbl(slow_task)
```

However, these commands only run in a single process, it means, if the list is doubled, the time is also at least doubled.

```{r}
system.time({
  list_of_vectors %>% map_dbl(slow_task)
})
# double the list
longer_list_of_vectors <- c(list_of_vectors, list_of_vectors)
system.time({
  longer_list_of_vectors %>% map_dbl(slow_task)
})
```

We are hoping to use multiple processes to speed up the job. The traditional way is to use the `parallel` package.

## The package `parallel`

```{r}
library(parallel)
```

Consider again the above list_vector example,
```{r}
# the number of cores we have
detectCores()
# it will create a socket cluster on my own computer
cl <- makeCluster(4)
parLapply(cl, list_of_vectors, slow_task)
# or if you want simplified result
parSapply(cl, list_of_vectors, slow_task)
# stop the cluster after use
stopCluster(cl)
```

Remark: you don't have to make and stop clusters for every operation, you could make a cluster in the very beginning of your script and close it at the very end.


Let's test the speed improvement

```{r}
run_each <- function(x, fun, n_cores) {
  cl <- makeCluster(n_cores)
  result <- parLapply(cl, x, fun)
  stopCluster(cl)
  result
}
system.time(run_each(longer_list_of_vectors, slow_task, 2))
system.time(run_each(longer_list_of_vectors, slow_task, 3))
system.time(run_each(longer_list_of_vectors, slow_task, 4))
```


## Processing Chunk

The iteratable is divided into chunks before sending the chunks to the workers. `Sys.getpid()` tells us the process id of a worker.

```{r}
cl <- makeCluster(4)
```

```{r}
parSapply(cl, 1:10, function(x) {
    Sys.getpid()
})

parSapply(cl, 1:10, function(x) {
    Sys.getpid()
  },
  chunk.size = 2
)

parSapply(cl, 1:10, function(x) {
    Sys.getpid()
  },
  chunk.size = 1
)
```

```{r}
stopCluster(cl)
```


## Load balancing


`parLapply` pre-schedules the tasks to each work. It could be suboptimal when different tasks require different amount of time to complete.

```{r}
cl <- makeCluster(4)
```

```{r}
x <- c(3, 3, 1, 1, 1, 1, 1, 1)
pause <- function(x) {
  Sys.sleep(x)
}
system.time({
  parLapply(cl, x, pause, chunk.size = 2)
})
system.time({
  parLapply(cl, x, pause, chunk.size = 1)
})
```

Instead of preshceduling the tasks, a task could also be assigned to a free worker dynamically using `parLapplyLB`.

```{r}
system.time({
  parLapplyLB(cl, x, pause)
})
```
Note that it only takes 3 seconds now.


```{r}
stopCluster(cl)
```



## Interact directly with the workers

We just saw an quick example on using `parLapply`. Let's try a few more things.

```{r}
cl <- makeCluster(4)
```

We could run some arbitrary commands on each of the workers
```{r}
clusterEvalQ(cl, {
  x <- rnorm(100)
  mean(x)
})
```

```{r}
clusterEvalQ(cl, {
  Sys.getpid()
})
```

Global variables in master are not exported to the worker automatically
```{r, error = TRUE}
y <- 3
clusterEvalQ(cl, {
  y + 1
})
```
`clusterExport` exports the global variables to each worker.
```{r}
clusterExport(cl, "y")
clusterEvalQ(cl, {
  y + 1
})
```


If you want to set a random seed, the following doesn't work because each work returns the same result.
```{r}
clusterEvalQ(cl, {
  set.seed(123)
  rnorm(5)
})
```

```{r}
clusterSetRNGStream(cl, 123)
clusterEvalQ(cl, {
  rnorm(5)
})
```


```{r}
# do not forget to close the cluster
stopCluster(cl)
```


## `map` or `lapply` like syntax

- `mclapply` from `parallel` (unix / macOS only)

Remark: `mclapply` relies on forking, it means that it doesn't work on Windows. We will discuss a cross platform approach.)

```{r}
list_of_vectors %>% mclapply(
  slow_task, 
  mc.cores = 4
)


list_of_vectors %>% mclapply(
  slow_task, 
  mc.preschedule = FALSE,  #  set FALSE to enable load balancing
  mc.cores = 4
)
```

- package `furrr`

`furrr` provides functions which are very similar to those in `purrr`.

```{r}
library(furrr)
```

```{r, warning = FALSE}
plan(multiprocess, workers = 4)
system.time({
  future_map(c(2, 2, 2, 2), ~Sys.sleep(.))
})
```

```{r}
future_map_dbl(list(1:10, 11:20, 21:30, 31:41), ~ sum(.))
```

How to do load balanacing with `future_map`?

```{r}
# without load balanacing
system.time({
  future_map(
    c(3, 3, 1, 1, 1, 1, 1, 1), 
    ~Sys.sleep(.))
})
# with load balanacing
system.time({
  future_map(
    c(3, 3, 1, 1, 1, 1, 1, 1), 
    ~Sys.sleep(.),
    .options = future_options(scheduling = FALSE))
})
```



## `foreach`



Reference:

- R Programming for Data Science https://bookdown.org/rdpeng/rprogdatascience/parallel-computation.html